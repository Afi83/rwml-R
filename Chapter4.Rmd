---
title: "Real-World Machine Learning (with R): Chapter 4"
author: "Paul Adamson"
date: "October 15, 2016"
output: html_document
---

This notebook contains R code to accompany Chapter 4 of the book 
["Real-World Machine Learning"](https://www.manning.com/books/real-world-machine-learning),
by  Henrik Brink, Joseph W. Richards, and Mark Fetherolf.  It is part of a 
series of R Markdown notebooks hosted on GitHub in the 
[rwml-R repo](https://github.com/padamson/rwml-R).

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/analytics/real-world-machine-learning-R/")
library(plyr)
library(dplyr)
#library(vcd)
library(AppliedPredictiveModeling)
library(caret)
#library(ellipse)
library(kknn)
library(RColorBrewer)
library(cowplot)
library(pROC)
#library(gridExtra)
#library(grid)
#library(randomForest)
set.seed(3456)
.pardefault <- par()
```

## Figure 4.9 The first five rows of the Titanic Passengers dataset 

As in Chapter 3, we are going to be interested in predicting survival, so again,
it is useful to specify 
the `Survived` variable to be of type `factor`. For visualizing the data, 
it is also useful to use the `revalue` function to specify the `no` and `yes`
levels for the `factor` variable. The `kable` function is built into the `knitr`
package.

```{r figure4.9, cache=TRUE}
titanic <- read.csv("data/titanic.csv", 
                    colClasses = c(
                      Survived = "factor",
                      Name = "character",
                      Ticket = "character",
                      Cabin = "character"))
titanic$Survived <- revalue(titanic$Survived, c("0"="no", "1"="yes"))
kable(head(titanic, 5), digits=2)
```

## Figure 4.10 Splitting the full dataset into training and testing sets

Here, we follow the same process used for Figure 3.6 to process the data and
prepare it for our model. First, we get rid of the variables that we do not 
want in our model.
(`Cabin` might actually be useful, but it's not used here.)
Then we use `is.na` to set missing age values to -1.
The `mutate` and `select` functions make it easy to take square root of 
the `Fare` variable and then drop it from the dataset.
We then drop rows with missing `Embarked` data and remove the unused level 
`""`. 
Finally, we convert `factor` variables to dummy variables using the 
`dummyVars` function in the `caret` package.
To avoid perfect collinearity (a.k.a. the dummy variable trap), we set
the `fullRank` parameter to `TRUE`.  `Survived.yes` is then converted back
to a `factor` variable.

We then make a 80/20% train/test split 
using the `Survived` factor
variable in the `createDataPartition` function to preserve the
overall class distribution of the data.

```{r figure4.10, cache=TRUE, dependson="figure4.9"}
titanicTidy <- subset(titanic, select = -c(PassengerId, Name, Ticket, Cabin))

titanicTidy$Age[is.na(titanicTidy$Age)] <- -1

titanicTidy <- titanicTidy %>%
  mutate(sqrtFare = sqrt(Fare)) %>%
  select(-Fare)

titanicTidy <- titanicTidy %>%
  filter(!(Embarked=="")) %>%
  droplevels

dummies <- dummyVars(" ~ .", data = titanicTidy, fullRank = TRUE)
titanicTidyNumeric <- data.frame(predict(dummies, newdata = titanicTidy))

titanicTidyNumeric$Survived.yes <- factor(titanicTidyNumeric$Survived.yes)

trainIndex <- createDataPartition(titanicTidyNumeric$Survived, p = .8, 
                                  list = FALSE, 
                                  times = 1)

titanicTrain <- titanicTidyNumeric[ trainIndex,]
titanicTest  <- titanicTidyNumeric[-trainIndex,]

kable(head(titanicTidyNumeric, 8), digits=2)
kable(head(titanicTrain, 5), digits=2)
kable(head(titanicTest, 3), digits=2)
```

## Figure 4.18 Handwritten digits in the MNIST dataset 

Thanks to [Longhow Lam](https://longhowlam.wordpress.com/2015/11/25/a-little-h2o-deeplearning-experiment-on-the-mnist-data-set/)
for posting the code used in the `displayMnistSamples` function that display's 
digits from the MNIST dataset.

```{r figure4.18, cache=TRUE,fig.height=5}
mnist <- read.csv("data/mnist_small.csv",
                  colClasses = c(label = "factor"))
displayMnistSamples <- function(x) {
  for(i in x){
  y = as.matrix(mnist[i, 2:785])
  dim(y) = c(28, 28)
  image( y[,nrow(y):1], axes = FALSE, col = gray(0:255 / 255))
  text( 0.2, 0, mnist[i,1], cex = 3, col = 2, pos = c(3,4))
  }
}
par( mfrow = c(4,5), mai = c(0,0,0.1,0.1))
displayMnistSamples(sample(1:length(mnist),20))
```

## Figure 4.19 The confusion matrix for the 10-class MNIST handwritten digit classification problem

```{r figure4.19, cache=TRUE, dependson="figure4.18", fig.height=4}
trainIndex <- createDataPartition(mnist$label, p = .8, 
                                  list = FALSE, 
                                  times = 1)
mnistTrain <- mnist[ trainIndex,]
mnistTest  <- mnist[-trainIndex,]

mnist.kknn <- kknn(label~., mnistTrain, mnistTest, distance = 1,
                   kernel = "triangular")

confusionDF <- data.frame(confusionMatrix(fitted(mnist.kknn),mnistTest$label)$table)
confusionDF$Reference = with(confusionDF, 
                             factor(Reference, levels = rev(levels(Reference))))

jBuPuFun <- colorRampPalette(brewer.pal(n = 9, "BuPu"))
paletteSize <- 256
jBuPuPalette <- jBuPuFun(paletteSize)

confusionPlot <- ggplot(
  confusionDF, aes(x = Prediction, y = Reference, fill = Freq)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
  geom_tile() +
  labs(x = "Predicted digit", y = "Actual digit") +
  scale_fill_gradient2(
    low = jBuPuPalette[1],
    mid = jBuPuPalette[paletteSize/2],
    high = jBuPuPalette[paletteSize],
    midpoint = (max(confusionDF$Freq) + min(confusionDF$Freq)) / 2,
    name = "") +
  theme(legend.key.height = unit(2, "cm"))

ggdraw(switch_axis_position(confusionPlot, axis = 'x'))
```

## Figure 4.20 The ROC curves for each class of the MNIST 10-class classifier

```{r figure4.20, cache=TRUE, dependson="figure"}
#mnist.kknn$prob
#roc(mnist.kknn$prob)
#ggplot(mnist.kknn)
```

